{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d75e751",
   "metadata": {},
   "source": [
    "<H1>Linear Tree Algorithm experiment</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988fefa4",
   "metadata": {},
   "source": [
    "1.) Collect, clean exemplary dataset for regression problem suited for linear tree model.<br>\n",
    "2.) Develop single tree algorithm with variables for depth, omitted features as input (others to be added later) and feature importance and classification solution as output<br>\n",
    "3.) Define simple Gradient Boosting algorithm<br>\n",
    "4.) Bring full algorithm together<br>\n",
    "5.) Run tests with exemplary dataset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc50f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly as pl\n",
    "import plotly.express as px\n",
    "\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6232e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning and pre-preparation completed in EDA module\n",
    "df=pd.read_csv('data/ames_housing_price_data_prepared_v01.csv')\n",
    "df['SalePriceLog']=np.log10(df['SalePrice'])\n",
    "df=df.drop(['Unnamed: 0','SalePrice'],axis=1)\n",
    "\n",
    "#price=df['SalePrice']\n",
    "#price_log = np.log10(price)\n",
    "#df.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4bd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduction of feature space to 15 features and dependent variable to reduce computational requirements.\n",
    "columnlist=['GrLivArea', 'LotArea', 'OverallQual', 'BSMT_LowQual',\n",
    "       'house_age_years', 'GarageCars', 'FullBath', 'HalfBath',\n",
    "       'BsmtExposure_ord', 'SaleTypeNew', 'PorchSF', 'BSMT_HighQual',\n",
    "       'Fireplaces', 'Pool', 'BedroomAbvGr', 'SalePriceLog']\n",
    "df=df[columnlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f14034",
   "metadata": {},
   "source": [
    "<H1>Initial Benchmark: XGBoost Regression model</H1><br>\n",
    "    Model has been optimized using a cross-validated Grid Search and tested on 30% of data separately afterwards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a845194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8860828156828164\n",
      "Params:  {'colsample_bytree': 1, 'gamma': 0, 'max_depth': 2, 'min_child_weight': 5, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('SalePriceLog',axis=1),df['SalePriceLog'],test_size=0.33)\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "params={\n",
    "        'min_child_weight': [1, 5, 7],\n",
    "        'gamma': [0,0.1, 0.5, 1],\n",
    "        'subsample': [0.5,1.0],\n",
    "        'colsample_bytree': [0.8,1],\n",
    "        'max_depth': [2,3,4,6,10]\n",
    "}\n",
    "\n",
    "grid=GridSearchCV(model, params,cv=3)\n",
    "grid.fit(X_train,y_train)\n",
    "print(f'Score: ',grid.score(X_test,y_test))\n",
    "print('Params: ',grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f986c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model performance at 88.61% as benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d50c2f",
   "metadata": {},
   "source": [
    "<H1>Tree Booster model with Ridge Regression as loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f2d4e",
   "metadata": {},
   "source": [
    "'''\n",
    "Overall setup:\n",
    "- Function that runs trees sequentially, predicting the previous tress residuals, and after each tree updates a 'Residuals' column with tree performance and learning rate\n",
    "- Function that calculates each tree, by iterating through all nodes, and for each node all columns and splitpoints within these colums. Tree is then always split by node that offers highest MSE-Reduction potential. Uses DP to save nodes that were already visited.\n",
    "- Function that calculates Ridge Regression coefficients for each node-split option, and MSE for the Ridge predictor.\n",
    "- Function that predicts Residuals and MSE for each tree after tree has been finalized\n",
    "- Function that finally allows to predict new datapoints.\n",
    "- Function to calculate R2 score for final treebooster model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd42d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBooster():\n",
    "    \n",
    "    def __init__(self, n_nodes=16, n_iterations=10, learning_rate=0.1, max_depth=6, min_elements=1,alpha=1):\n",
    "        '''\n",
    "        Initialize class and save key parameters for booster. Initialize dictionaries/ lists to store \n",
    "        split_conditions during training, and linear regression coefficienys for each exit node.\n",
    "        '''\n",
    "        self.n_nodes=n_nodes\n",
    "        self.n_iterations=n_iterations\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.min_elements=min_elements\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.split_conditions={}\n",
    "        self.tree_coefs={}\n",
    "        self.tree_depth={}\n",
    "        self.columns=[]\n",
    "        \n",
    "        self.initial_mean=0\n",
    "        \n",
    "        #feature_importance_dict={}\n",
    "        \n",
    "    def ridge_coefs(self,df):\n",
    "        '''\n",
    "        #based on dependent and predictor columns, run RidgeRegression fit and score for each       \n",
    "        Input: dataframe with relevant rows\n",
    "        Output: List of coefficients beta_0 in first position, beta_1 to beta_n following\n",
    "        '''\n",
    "\n",
    "        onerow=np.ones(df.shape[0]).reshape(df.shape[0],1)\n",
    "        \n",
    "        #Split data into predictor and dependent vaiables\n",
    "        X_vals=np.c_[onerow,np.matrix(df[self.columns])]\n",
    "        Y_vals=np.matrix(df['Residuals'])\n",
    "        Identity=np.eye(X_vals.shape[1])\n",
    "        Identity[0,0]=0\n",
    "        \n",
    "        #Ridge solution for coefficients: ùõΩ=(ùëãùëáùëã+ùúÜùêº)‚àí1ùëãùëáùë¶.\n",
    "        coefs=(X_vals.T.dot(X_vals)+self.alpha*Identity).I.dot(X_vals.T).dot(Y_vals.T)\n",
    "        return coefs\n",
    "    \n",
    "    def check_error(self,df):\n",
    "        '''\n",
    "        For a subset dataframe, calculate the error (currently only setup for MSE) of the potential leaf node.\n",
    "        Input: dataframe with relevant rows\n",
    "        Output: Error as float (MSE)\n",
    "        '''\n",
    "        \n",
    "        #Collect Ridge regression coefficients from function ridge_coefs()\n",
    "        coefs=self.ridge_coefs(df)\n",
    "        error=sum(np.square(np.matrix(df['Residuals']).T-(np.matrix(df[self.columns])*coefs[1:]+coefs[0])))/df.shape[0]\n",
    "        return error\n",
    "\n",
    "    def treebuilder(self, df, tree_num=0):\n",
    "        '''\n",
    "        Create a decision tree, by iterating each node, testing all splits for each feature and each \n",
    "        split_value within the feature, then split the tree based on the feature generating highest MSE-reduction\n",
    "        Input: Dataframe with a column called 'Residuals' as dependent variable, and all columns as defined\n",
    "        in class variable self.columns as Predictor variables.\n",
    "        Output: Updated split_conditions dicitionary, providing for each node the next split conditions, and\n",
    "        subnote_ids. Can be used to run a full prediction based on the tree splitting criteria.\n",
    "        '''\n",
    "        \n",
    "        #Set each observation to tree_node 0 (this will be adjusted once we start splitting the tree)\n",
    "        df['subtree_id']=0\n",
    "        \n",
    "        max_subtree_id=0\n",
    "        \n",
    "        #Set tmp_dict to save visited nodes (subtree_ids) and their loss-reduction performance.\n",
    "        tmp_dict={}\n",
    "        \n",
    "        #Initialize dictionaries to save split conditions, and coefficients for leaf regression for each node.\n",
    "        self.split_conditions[tree_num]={}\n",
    "        self.tree_coefs[tree_num]={}\n",
    "        self.tree_depth[tree_num]={}\n",
    "        \n",
    "        self.tree_depth[tree_num][0]=0\n",
    "\n",
    "        k=0\n",
    "        \n",
    "        #Execute for specified number of nodes\n",
    "        while k<self.n_nodes:\n",
    "\n",
    "            subtree_ids=df['subtree_id'].unique()\n",
    "            \n",
    "            for subtree_id in subtree_ids: #iterate through each current subtree\n",
    "                \n",
    "                #stop, if max_depth for node already reached\n",
    "                if self.tree_depth[tree_num][subtree_id]<self.max_depth:\n",
    "                    \n",
    "                    currentError=self.check_error(df[df['subtree_id']==subtree_id])\n",
    "                    \n",
    "                    for column in self.columns:#Iterate each feature\n",
    "\n",
    "                        if (subtree_id,column) not in tmp_dict:#Check if we already checked this node previously\n",
    "                            \n",
    "                            #Define split values as the average between each two observations in a sorted range\n",
    "                            split_values=list(df[df['subtree_id']==subtree_id][column].unique())\n",
    "                            split_values.sort()\n",
    "                            split_val=[]\n",
    "                            for i in range(1, len(split_values)):\n",
    "                                split_val.append(split_values[i-1]+(split_values[i]-split_values[i-1])/2)\n",
    "\n",
    "                            maxError=0\n",
    "                            total_shape=df[(df['subtree_id']==subtree_id)].shape[0]\n",
    "                            \n",
    "                            for i in split_val:#iterate through each split\n",
    "                                \n",
    "                                bottom_shape=df[(df['subtree_id']==subtree_id) & (df[column]<i)].shape[0]\n",
    "                                top_shape=df[(df['subtree_id']==subtree_id) & (df[column]>=i)].shape[0]\n",
    "                                \n",
    "                                if bottom_shape>=self.min_elements and top_shape>=self.min_elements:\n",
    "                                    \n",
    "                                    #Check MSE for each resuliting sub_node for given split and calculate weighted average\n",
    "                                    newError=(self.check_error(df[(df['subtree_id']==subtree_id) & (df[column]<i)])*bottom_shape + self.check_error(df[(df['subtree_id']==subtree_id) & (df[column]>=i)])*top_shape)/total_shape\n",
    "\n",
    "                                    deltaError=currentError-newError\n",
    "\n",
    "                                    #if DeltaError is larger than maxError for this node/feature combination, save it to consider it for future split\n",
    "                                    if deltaError>maxError:\n",
    "                                        tmp_dict[(subtree_id,column)]=(i,deltaError)\n",
    "                                        maxError=deltaError\n",
    "            \n",
    "            #Check if we have tested any splits (could be no, if e.g. all nodes at max_depth)\n",
    "            if tmp_dict:\n",
    "                \n",
    "                #Collect best split from tmp_dict, and set subtree_id in dataframe to two new values for newly defined split\n",
    "                best_subtree_id, best_column=max(tmp_dict, key=lambda x: tmp_dict.get(x)[1])\n",
    "                best_split_pos=tmp_dict[(best_subtree_id, best_column)][0]\n",
    "                max_subtree_id+=1\n",
    "                df.loc[(df['subtree_id']==best_subtree_id) & (df[best_column]<best_split_pos),'subtree_id']=max_subtree_id\n",
    "                self.tree_depth[tree_num][max_subtree_id]=self.tree_depth[tree_num][best_subtree_id]+1\n",
    "                max_subtree_id+=1\n",
    "                df.loc[(df['subtree_id']==best_subtree_id)&(df[best_column]>=best_split_pos),'subtree_id']=max_subtree_id\n",
    "                self.tree_depth[tree_num][max_subtree_id]=self.tree_depth[tree_num][best_subtree_id]+1\n",
    "                \n",
    "                #Delete all keys from tmp dict that are now invalid since we split the node they checked.\n",
    "                for i,j in list(tmp_dict.keys()):\n",
    "                    if i==best_subtree_id:\n",
    "                        del tmp_dict[(i,j)]   \n",
    "                        \n",
    "                #Save splitting conditions        \n",
    "                self.split_conditions[tree_num][best_subtree_id]=(best_column,best_split_pos,max_subtree_id-1,max_subtree_id)\n",
    "            \n",
    "            k+=1\n",
    "            \n",
    "            #print(df['subtree_id'].value_counts())\n",
    "\n",
    "        #calculate ridge regression coefficients for each subtree, use coefficients when predicting datapoints\n",
    "        subtree_ids=df['subtree_id'].unique()\n",
    "        for subtree_id in subtree_ids: #each subtree\n",
    "            coefs=self.ridge_coefs(df[df['subtree_id']==subtree_id])\n",
    "            self.tree_coefs[tree_num][subtree_id]=coefs\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def tree_predictor(self, df, tree_num=0):\n",
    "        '''\n",
    "        Predict the dependent variable for a single tree based on the previously defined tree-logic.\n",
    "        Input: dataframe with all training data and split_conditions dictionary outlining how to split the tree.\n",
    "        Output: Dataframe with attached column 'Predict' containing the predictions\n",
    "        '''\n",
    "        df['Predict']=0\n",
    "        \n",
    "        for i in df.index:\n",
    "            current_subtree_id=0\n",
    "            while current_subtree_id not in self.tree_coefs[tree_num]:\n",
    "                column,split_pos,id_left,id_right = self.split_conditions[tree_num][current_subtree_id]\n",
    "                if df[column][i]<split_pos:\n",
    "                    current_subtree_id=id_left\n",
    "                else:\n",
    "                    current_subtree_id=id_right\n",
    "                    \n",
    "            coefs=self.tree_coefs[tree_num][current_subtree_id]\n",
    "            prediction=float(np.matrix(df.loc[i,self.columns])*coefs[1:]+coefs[0])\n",
    "            df.loc[i,'Predict']=prediction\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def iteration_builder(self, df):\n",
    "        '''\n",
    "        Booster logic: Initialize fitting procedure by setting the base prediction as the mean of all valus, then\n",
    "        start predicting and iteratively updating the residuals tree by tree.\n",
    "        Input: Dataframe with Predictor column named 'SalesPriceLog' (should be generalized later) as the last \n",
    "        column in the dataframe, and all previous columns as predictor features.\n",
    "        Output: Split_conditions dictionary, putlinign for each tree the split conditions\n",
    "        '''\n",
    "        df2=df.copy()\n",
    "        \n",
    "        #Define all predictor columns and save into class\n",
    "        self.columns=list(df2.columns)\n",
    "        self.columns.pop()\n",
    "        \n",
    "        #Initialze model by setting prediction to mean of all dependent variable observations\n",
    "        df2['Base_prediction']=df2['SalePriceLog'].mean()\n",
    "        self.initial_mean=df2['SalePriceLog'].mean()\n",
    "        df2['Residuals']=df2['SalePriceLog']-df2['Base_prediction']\n",
    "        \n",
    "        res_mean=df2['Residuals'].map(lambda x: x**2).mean()\n",
    "        print(f'Starting; residual mean at {res_mean}')\n",
    "        \n",
    "        #Build trees and update residuals based on each tree\n",
    "        for x in range(self.n_iterations):\n",
    "            df2=self.treebuilder(df2,tree_num=x)\n",
    "            df2=self.tree_predictor(df2,tree_num=x)\n",
    "            df2['Base_prediction']=df2['Base_prediction']+df2['Predict']*self.learning_rate\n",
    "            df2['Residuals']=df2['SalePriceLog']-df2['Base_prediction']\n",
    "            df2=df2.drop(['Predict'],axis=1)\n",
    "            \n",
    "            res_mean=df2['Residuals'].map(lambda x: x**2).mean()\n",
    "            print(f'Iteration {x} complete, residual mean at {res_mean}')\n",
    "        return df2\n",
    "    \n",
    "    def iteration_predictor(self, df):\n",
    "        '''\n",
    "        Predict dependent variable for new observations by sequentially creating each tree, and calculating \n",
    "        new residual based on the new tree.\n",
    "        Input: Dataframe with each observation\n",
    "        Output: Dataframe with extra column containing Predicted values for each observation.\n",
    "        '''\n",
    "        df2=df.copy()\n",
    "        df2=df2.reset_index(drop=False)\n",
    "\n",
    "        df2['Predict_fin']=self.initial_mean\n",
    "        for x in range(self.n_iterations):\n",
    "            df2=self.tree_predictor(df2,tree_num=x)\n",
    "            df2['Predict_fin']=df2['Predict_fin']+self.learning_rate*df2['Predict']\n",
    "            \n",
    "        df2['Predict']=df2['Predict_fin']\n",
    "        df2=df2.drop('Predict_fin',axis=1)\n",
    "        \n",
    "        df2=df2.set_index('index',drop=True)\n",
    "        df2.index.name=None\n",
    "\n",
    "        return df2\n",
    "    \n",
    "    def tree_scorer(self,df):\n",
    "        '''\n",
    "        Calculate R2 score for any dataframe.\n",
    "        Input: Dataframe with two columns: Depedent variable and predicted values per observation\n",
    "        Output: R2 score\n",
    "        '''\n",
    "        mean1=sum(df.iloc[:,0])/df.shape[0]\n",
    "        TSS=sum(df.iloc[:,0].map(lambda x: (x-mean1)**2))\n",
    "\n",
    "        RSS=sum((df.iloc[:,1]-df.iloc[:,0])**2)\n",
    "\n",
    "        return 1-RSS/TSS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c12ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchCV(df):\n",
    "    '''\n",
    "    Basic GridSearch function: For now, alywas uses #3 splits, each based on random bagging of 67% \n",
    "    of observations in train for Cross-Validation.\n",
    "    Currently set to always use same GridSeatch Options and model (can be restructured as inputs later)\n",
    "    Input: Dataframe with train data (last column should be dependent variable, with name 'SalePriceLog')\n",
    "    Output: Average cross-validation test score and parameters for best parameter combination\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    scores={}\n",
    "    \n",
    "    options={'n_nodes': [8,10],\n",
    "             'n_iterations': [25],\n",
    "             'learning_rate': [0.1],\n",
    "             'max_depth': [3,4],\n",
    "             'min_elements': [10,20], \n",
    "             'alpha': [0.1,20]\n",
    "            }\n",
    "    \n",
    "    iterables=list(itertools.product(options['n_nodes'],options['n_iterations'],options['learning_rate'],options['max_depth'],options['min_elements'],options['alpha']))\n",
    "    \n",
    "    for n_nodes,n_iterations,learning_rate,max_depth,min_elements,alpha in iterables:\n",
    "        \n",
    "        tmpscore=[]\n",
    "        \n",
    "        for _ in range(3):\n",
    "    \n",
    "            train_indices=list(np.random.choice(df.index, size=int(df.shape[0]*0.667),replace=False))\n",
    "            test_indices=[x if x not in train_indices else -1 for x in df.index]\n",
    "            d_train=df.loc[df.index.isin(train_indices)]\n",
    "            d_test=df.loc[df.index.isin(test_indices)]\n",
    "    \n",
    "    \n",
    "            treemodel=TreeBooster(n_nodes=n_nodes, n_iterations=n_iterations, learning_rate=learning_rate, max_depth=max_depth, min_elements=min_elements,alpha=alpha)\n",
    "            d_train= treemodel.iteration_builder(d_train)\n",
    "    \n",
    "            d_test=d_test.reset_index(drop=False)\n",
    "            d_test2=treemodel.iteration_predictor(d_test)\n",
    "            score=treemodel.tree_scorer(d_test2[['SalePriceLog','Predict']])\n",
    "            \n",
    "            tmpscore.append(score)\n",
    "            \n",
    "        scores[sum(tmpscore)/len(tmpscore)]=(n_nodes,n_iterations,learning_rate,max_depth,min_elements,alpha)\n",
    "        \n",
    "        print(scores[max(scores.keys())], max(scores.keys()))\n",
    "    \n",
    "    return scores[max(scores.keys())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8899fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices=list(np.random.choice(df.index, size=int(df.shape[0]*0.7),replace=False))   #int(df.shape[0]*0.7)\n",
    "test_indices=[x if x not in train_indices else -1 for x in df.index]\n",
    "df_train=df.loc[df.index.isin(train_indices)]\n",
    "df_test=df.loc[df.index.isin(test_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82880319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_nodes,n_iterations,learning_rate,max_depth,min_elements,alpha = GridSearchCV(df_train)\n",
    "#print(n_nodes,n_iterations,learning_rate,max_depth,min_elements,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3ab082",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes,n_iterations,learning_rate,max_depth,min_elements,alpha = (6, 25, 0.1, 4, 40, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting; residual mean at 0.026840117957575325\n",
      "Iteration 0 complete, residual mean at 0.02212114925739094\n",
      "Iteration 1 complete, residual mean at 0.01828673982825497\n",
      "Iteration 2 complete, residual mean at 0.015211318434035068\n",
      "Iteration 3 complete, residual mean at 0.012670886238429789\n",
      "Iteration 4 complete, residual mean at 0.010615896673640674\n",
      "Iteration 5 complete, residual mean at 0.008950710433264224\n",
      "Iteration 6 complete, residual mean at 0.007580763600133983\n",
      "Iteration 7 complete, residual mean at 0.006472235242499397\n",
      "Iteration 8 complete, residual mean at 0.00556409778842203\n",
      "Iteration 9 complete, residual mean at 0.004830306399768823\n",
      "Iteration 10 complete, residual mean at 0.004217527538948213\n",
      "Iteration 11 complete, residual mean at 0.0037161693143886827\n",
      "Iteration 12 complete, residual mean at 0.003305164890852075\n",
      "Iteration 13 complete, residual mean at 0.002967973426617146\n",
      "Iteration 14 complete, residual mean at 0.0026964481080186302\n",
      "Iteration 15 complete, residual mean at 0.002469653613670047\n",
      "Iteration 16 complete, residual mean at 0.002274447440032727\n",
      "Iteration 17 complete, residual mean at 0.0021191052211107972\n"
     ]
    }
   ],
   "source": [
    "treemodel=TreeBooster(n_nodes=n_nodes, n_iterations=n_iterations, learning_rate=learning_rate, max_depth=max_depth, min_elements=min_elements,alpha=alpha)\n",
    "df_train= treemodel.iteration_builder(df_train)\n",
    "\n",
    "df_test=df_test.reset_index(drop=False)\n",
    "df_test2=treemodel.iteration_predictor(df_test)\n",
    "treemodel.tree_scorer(df_test2[['SalePriceLog','Predict']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without Ridge: 0.9024810993000432\n",
    "#XGBoost fully optimized: 0.90325821780744\n",
    "#Basic Linear Regression (not penalized): 0.8974280280636848\n",
    "\n",
    "#with Ridge, 400 obs, not optimized: 0.8073027768102685\n",
    "#Ridge, all obs, not optimized: 0.8651592190813793\n",
    "#Ridge 0.9091286769287041 #(n_iter=6, n_iterations=25, learning_rate=0.1, max_depth=4, min_elements=30, alpha=0.1)\n",
    "#0.9108590610477793 at (n_iter=6, n_iterations=25, learning_rate=0.1, max_depth=4, min_elements=40, alpha=0.001)\n",
    "#0.9033206921634097 (8, 25, 0.1, 4, 20, 0.1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
